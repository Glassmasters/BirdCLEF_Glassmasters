{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libs\n",
    "import os\n",
    "%matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_BASE_FILE_PATH = r\"D:\\Datasets\\birdclef-2023\"\n",
    "TRAIN_SET_FILE_DIR = r\"\\train_audio\"\n",
    "TEST_SET_FILE_DIR = r\"\\test_soundscapes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(DATASET_BASE_FILE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(DATASET_BASE_FILE_PATH + TRAIN_SET_FILE_DIR))\n",
    "print(f\"Number of test samples: {len(os.listdir(DATASET_BASE_FILE_PATH + TRAIN_SET_FILE_DIR))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(DATASET_BASE_FILE_PATH + TEST_SET_FILE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of .csv files in base dir\n",
    "- sample_submission.csv\n",
    "- eBird_Taxonomy_v2021.csv\n",
    "- train_metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with the taxonomy csv file\n",
    "taxonomy = pd.read_csv(DATASET_BASE_FILE_PATH + \"\\\\eBird_Taxonomy_v2021.csv\")\n",
    "\n",
    "print(list(taxonomy.columns))\n",
    "print(list(taxonomy.dtypes))\n",
    "\n",
    "print(f\"Shape of dataframe (rows, columns): {taxonomy.shape}\")\n",
    "\n",
    "# percentage of NANs in each column\n",
    "print(taxonomy.isnull().sum(axis = 0)/taxonomy.shape[0])\n",
    "\n",
    "# Unique values per column\n",
    "print(taxonomy.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(taxonomy.head(3))\n",
    "print(taxonomy.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closer look into category column\n",
    "print(list(set(taxonomy['CATEGORY'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the pandas profiling report\n",
    "profile_taxonomy = ProfileReport(taxonomy, title=\"Pandas Profiling Report - Taxonomy\")\n",
    "profile_taxonomy.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of taxonomy csv\n",
    "- 9 columns, ca. 17k rows\n",
    "- Littel non-NAN entires in SPECIES_GROUP and REPORT_AS\n",
    "- SPECIES_CODE could be used to get more infomration from https://ebird.org/species/SPECIES_CODE\n",
    "- Everything else is a black box for me currently\n",
    "\n",
    "#### Open questions\n",
    "- What exactly is the TAXON_ORDER?\n",
    "- What are the categories in the CATEGORY column besides species?\n",
    "- How to use this taxonomy information?\n",
    "    - To combine / seperate data of species living close to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of train metadata csv\n",
    "train_metadata = pd.read_csv(DATASET_BASE_FILE_PATH + \"\\\\train_metadata.csv\")\n",
    "\n",
    "print(list(train_metadata.columns))\n",
    "print(list(train_metadata.dtypes))\n",
    "\n",
    "print(f\"Shape of dataframe (rows, columns): {train_metadata.shape}\")\n",
    "\n",
    "# percentage of NANs in each column\n",
    "print(train_metadata.isnull().sum(axis = 0)/train_metadata.shape[0])\n",
    "\n",
    "# Unique values per column\n",
    "print(train_metadata.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_metadata.head(3))\n",
    "print(train_metadata.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the pandas profiling report\n",
    "profile_train_metadata = ProfileReport(train_metadata, title=\"Pandas Profiling Report - Train metadata\")\n",
    "profile_train_metadata.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate results\n",
    "- 16941 rows, 12 columns\n",
    "- Little NAN entries accross all columns\n",
    "\n",
    "#### Open questions\n",
    "- How to use the extra information?\n",
    "    - As far as I understood it we do NOT have such metadata in the inference case\n",
    "- Scientific name the same as in the taxonomy file? --> We could join on this column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of train set files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dirs_in_train_dir = os.listdir(DATASET_BASE_FILE_PATH + TRAIN_SET_FILE_DIR)\n",
    "number_of_files_for_single_sample = []\n",
    "\n",
    "for single_dir in list_of_dirs_in_train_dir:\n",
    "    number_of_files = len(os.listdir(DATASET_BASE_FILE_PATH + TRAIN_SET_FILE_DIR + \"\\\\\" + single_dir))\n",
    "    number_of_files_for_single_sample.append(number_of_files)\n",
    "\n",
    "print(number_of_files_for_single_sample)\n",
    "print(\n",
    "    f\"Max. samples: {min(number_of_files_for_single_sample)}, \\\n",
    "    Min. samples: {max(number_of_files_for_single_sample)}, \\\n",
    "    Mean samples: {sum(number_of_files_for_single_sample)/len(number_of_files_for_single_sample)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution per training samples to get a feeling for the balance of the training set\n",
    "plt.bar(list_of_dirs_in_train_dir, number_of_files_for_single_sample)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bars after sorting and the cummulative sum in one plot\n",
    "number_of_files_for_single_sample_sorted, list_of_dirs_in_train_dir_sorted = map(list, zip(*sorted(zip(number_of_files_for_single_sample, list_of_dirs_in_train_dir), reverse=True)))\n",
    "\n",
    "cum_sum_samples = np.cumsum(number_of_files_for_single_sample_sorted)\n",
    "total_file_sum = sum(number_of_files_for_single_sample_sorted)\n",
    "cum_sum_samples = np.divide(cum_sum_samples, np.repeat(total_file_sum, len(cum_sum_samples)))\n",
    "\n",
    "fig, ax1 = plt.subplots() \n",
    "ax1.set_xlabel('Train samples') \n",
    "ax1.set_ylabel('Cumulative sum', color = 'red') \n",
    "ax1.plot(list_of_dirs_in_train_dir_sorted, cum_sum_samples, color = 'red') \n",
    "ax1.tick_params(axis ='y', labelcolor = 'red') \n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Samples', color = 'blue') \n",
    "ax2.bar(list_of_dirs_in_train_dir_sorted, number_of_files_for_single_sample_sorted, color = 'blue') \n",
    "ax2.tick_params(axis ='y', labelcolor = 'blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate results\n",
    "\n",
    "- Training dataset is skewed\n",
    "    - Long tail of training classes with less than 10 examples. Even a few training classes with only <b>one</b> samples\n",
    "    - Maximum samples of training classes is 500\n",
    "- <b>Need to account for skewness</b>\n",
    "    - Important for training/validation split --> stratification needed if data is used as is\n",
    "    - Downsampling might not be a good idea as we throw away up to 499 samples of some classes\n",
    "    - How do we upsample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
