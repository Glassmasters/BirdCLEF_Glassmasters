{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "# import necessary libs\n",
    "import os\n",
    "%matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_BASE_FILE_PATH = r\"D:\\Datasets\\birdclef-2023\"\n",
    "TRAIN_SET_FILE_DIR = r\"\\train_audio\"\n",
    "TEST_SET_FILE_DIR = r\"\\test_soundscapes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eBird_Taxonomy_v2021.csv', 'sample_submission.csv', 'test_soundscapes', 'train_audio', 'train_metadata.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(DATASET_BASE_FILE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abethr1', 'abhori1', 'abythr1', 'afbfly1', 'afdfly1', 'afecuc1', 'affeag1', 'afgfly1', 'afghor1', 'afmdov1', 'afpfly1', 'afpkin1', 'afpwag1', 'afrgos1', 'afrgrp1', 'afrjac1', 'afrthr1', 'amesun2', 'augbuz1', 'bagwea1', 'barswa', 'bawhor2', 'bawman1', 'bcbeat1', 'beasun2', 'bkctch1', 'bkfruw1', 'blacra1', 'blacuc1', 'blakit1', 'blaplo1', 'blbpuf2', 'blcapa2', 'blfbus1', 'blhgon1', 'blhher1', 'blksaw1', 'blnmou1', 'blnwea1', 'bltapa1', 'bltbar1', 'bltori1', 'blwlap1', 'brcale1', 'brcsta1', 'brctch1', 'brcwea1', 'brican1', 'brobab1', 'broman1', 'brosun1', 'brrwhe3', 'brtcha1', 'brubru1', 'brwwar1', 'bswdov1', 'btweye2', 'bubwar2', 'butapa1', 'cabgre1', 'carcha1', 'carwoo1', 'categr', 'ccbeat1', 'chespa1', 'chewea1', 'chibat1', 'chtapa3', 'chucis1', 'cibwar1', 'cohmar1', 'colsun2', 'combul2', 'combuz1', 'comsan', 'crefra2', 'crheag1', 'crohor1', 'darbar1', 'darter3', 'didcuc1', 'dotbar1', 'dutdov1', 'easmog1', 'eaywag1', 'edcsun3', 'egygoo', 'equaka1', 'eswdov1', 'eubeat1', 'fatrav1', 'fatwid1', 'fislov1', 'fotdro5', 'gabgos2', 'gargan', 'gbesta1', 'gnbcam2', 'gnhsun1', 'gobbun1', 'gobsta5', 'gobwea1', 'golher1', 'grbcam1', 'grccra1', 'grecor', 'greegr', 'grewoo2', 'grwpyt1', 'gryapa1', 'grywrw1', 'gybfis1', 'gycwar3', 'gyhbus1', 'gyhkin1', 'gyhneg1', 'gyhspa1', 'gytbar1', 'hadibi1', 'hamerk1', 'hartur1', 'helgui', 'hipbab1', 'hoopoe', 'huncis1', 'hunsun2', 'joygre1', 'kerspa2', 'klacuc1', 'kvbsun1', 'laudov1', 'lawgol', 'lesmaw1', 'lessts1', 'libeat1', 'litegr', 'litswi1', 'litwea1', 'loceag1', 'lotcor1', 'lotlap1', 'luebus1', 'mabeat1', 'macshr1', 'malkin1', 'marsto1', 'marsun2', 'mcptit1', 'meypar1', 'moccha1', 'mouwag1', 'ndcsun2', 'nobfly1', 'norbro1', 'norcro1', 'norfis1', 'norpuf1', 'nubwoo1', 'pabspa1', 'palfly2', 'palpri1', 'piecro1', 'piekin1', 'pitwhy', 'purgre2', 'pygbat1', 'quailf1', 'ratcis1', 'raybar1', 'rbsrob1', 'rebfir2', 'rebhor1', 'reboxp1', 'reccor', 'reccuc1', 'reedov1', 'refbar2', 'refcro1', 'reftin1', 'refwar2', 'rehblu1', 'rehwea1', 'reisee2', 'rerswa1', 'rewsta1', 'rindov', 'rocmar2', 'rostur1', 'ruegls1', 'rufcha2', 'sacibi2', 'sccsun2', 'scrcha1', 'scthon1', 'shesta1', 'sichor1', 'sincis1', 'slbgre1', 'slcbou1', 'sltnig1', 'sobfly1', 'somgre1', 'somtit4', 'soucit1', 'soufis1', 'spemou2', 'spepig1', 'spewea1', 'spfbar1', 'spfwea1', 'spmthr1', 'spwlap1', 'squher1', 'strher', 'strsee1', 'stusta1', 'subbus1', 'supsta1', 'tacsun1', 'tafpri1', 'tamdov1', 'thrnig1', 'trobou1', 'varsun2', 'vibsta2', 'vilwea1', 'vimwea1', 'walsta1', 'wbgbir1', 'wbrcha2', 'wbswea1', 'wfbeat1', 'whbcan1', 'whbcou1', 'whbcro2', 'whbtit5', 'whbwea1', 'whbwhe3', 'whcpri2', 'whctur2', 'wheslf1', 'whhsaw1', 'whihel1', 'whrshr1', 'witswa1', 'wlwwar', 'wookin1', 'woosan', 'wtbeat1', 'yebapa1', 'yebbar1', 'yebduc1', 'yebere1', 'yebgre1', 'yebsto1', 'yeccan1', 'yefcan', 'yelbis1', 'yenspu1', 'yertin1', 'yesbar1', 'yespet1', 'yetgre1', 'yewgre1']\n",
      "Number of test samples: 264\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(DATASET_BASE_FILE_PATH + TRAIN_SET_FILE_DIR))\n",
    "print(f\"Number of test samples: {len(os.listdir(DATASET_BASE_FILE_PATH + TRAIN_SET_FILE_DIR))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soundscape_29201.ogg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.listdir(DATASET_BASE_FILE_PATH + TEST_SET_FILE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of .csv files in base dir\n",
    "- sample_submission.csv\n",
    "- eBird_Taxonomy_v2021.csv\n",
    "- train_metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of train set files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 126, 28, 18, 31, 90, 48, 8, 72, 37, 104, 1, 81, 57, 25, 30, 45, 43, 12, 24, 500, 47, 7, 81, 34, 109, 28, 60, 76, 262, 50, 166, 22, 38, 23, 16, 13, 26, 17, 11, 7, 20, 15, 10, 3, 62, 2, 29, 22, 38, 30, 8, 1, 81, 40, 27, 67, 9, 30, 34, 153, 43, 166, 13, 6, 8, 79, 27, 29, 113, 425, 181, 293, 477, 500, 1, 36, 49, 32, 7, 79, 3, 8, 15, 500, 21, 152, 7, 63, 437, 25, 5, 15, 137, 34, 136, 45, 239, 19, 81, 3, 5, 2, 94, 12, 138, 252, 103, 26, 28, 20, 10, 51, 72, 53, 24, 46, 9, 129, 30, 18, 59, 8, 436, 16, 5, 7, 34, 56, 9, 109, 68, 14, 40, 29, 378, 72, 18, 15, 1, 3, 19, 16, 6, 15, 6, 40, 8, 32, 36, 30, 24, 16, 20, 21, 20, 20, 22, 7, 14, 4, 91, 121, 52, 10, 14, 44, 172, 17, 281, 47, 28, 21, 56, 122, 70, 12, 42, 59, 9, 2, 5, 25, 227, 28, 116, 22, 10, 33, 6, 6, 78, 30, 25, 3, 45, 68, 23, 51, 33, 21, 199, 22, 21, 42, 41, 37, 32, 13, 12, 94, 59, 48, 119, 33, 4, 90, 37, 6, 161, 97, 500, 98, 105, 19, 88, 18, 34, 27, 132, 78, 48, 10, 73, 7, 23, 19, 34, 9, 1, 17, 1, 14, 8, 5, 500, 67, 486, 28, 106, 34, 17, 35, 22, 1, 13, 75, 19, 12, 134, 29, 15, 27, 108]\n",
      "Max. samples: 1,     Min. samples: 500,     Mean samples: 64.17045454545455\n"
     ]
    }
   ],
   "source": [
    "list_of_dirs_in_train_dir = os.listdir(DATASET_BASE_FILE_PATH + TRAIN_SET_FILE_DIR)\n",
    "number_of_files_for_single_sample = []\n",
    "\n",
    "for single_dir in list_of_dirs_in_train_dir:\n",
    "    number_of_files = len(os.listdir(DATASET_BASE_FILE_PATH + TRAIN_SET_FILE_DIR + \"\\\\\" + single_dir))\n",
    "    number_of_files_for_single_sample.append(number_of_files)\n",
    "\n",
    "print(number_of_files_for_single_sample)\n",
    "print(\n",
    "    f\"Max. samples: {min(number_of_files_for_single_sample)}, \\\n",
    "    Min. samples: {max(number_of_files_for_single_sample)}, \\\n",
    "    Mean samples: {sum(number_of_files_for_single_sample)/len(number_of_files_for_single_sample)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution per training samples to get a feeling for the balance of the training set\n",
    "plt.bar(list_of_dirs_in_train_dir, number_of_files_for_single_sample)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bars after sorting and the cummulative sum in one plot\n",
    "number_of_files_for_single_sample_sorted, list_of_dirs_in_train_dir_sorted = map(list, zip(*sorted(zip(number_of_files_for_single_sample, list_of_dirs_in_train_dir), reverse=True)))\n",
    "\n",
    "cum_sum_samples = np.cumsum(number_of_files_for_single_sample_sorted)\n",
    "total_file_sum = sum(number_of_files_for_single_sample_sorted)\n",
    "cum_sum_samples = np.divide(cum_sum_samples, np.repeat(total_file_sum, len(cum_sum_samples)))\n",
    "\n",
    "fig, ax1 = plt.subplots() \n",
    "ax1.set_xlabel('Train samples') \n",
    "ax1.set_ylabel('Cumulative sum', color = 'red') \n",
    "ax1.plot(list_of_dirs_in_train_dir_sorted, cum_sum_samples, color = 'red') \n",
    "ax1.tick_params(axis ='y', labelcolor = 'red') \n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Samples', color = 'blue') \n",
    "ax2.bar(list_of_dirs_in_train_dir_sorted, number_of_files_for_single_sample_sorted, color = 'blue') \n",
    "ax2.tick_params(axis ='y', labelcolor = 'blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate results\n",
    "\n",
    "- Training dataset is skewed\n",
    "    - Long tail of training classes with less than 10 examples. Even a few training classes with only <b>one</b> samples\n",
    "    - Maximum samples of training classes is 500\n",
    "- <b>Need to account for skewness</b>\n",
    "    - Important for training/validation split --> stratification needed if data is used as is\n",
    "    - Downsampling might not be a good idea as we throw away up to 499 samples of some classes\n",
    "    - How do we upsample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
